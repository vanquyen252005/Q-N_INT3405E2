{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T14:44:16.803325Z",
     "iopub.status.busy": "2025-12-09T14:44:16.802688Z",
     "iopub.status.idle": "2025-12-09T14:44:16.809638Z",
     "shell.execute_reply": "2025-12-09T14:44:16.809040Z",
     "shell.execute_reply.started": "2025-12-09T14:44:16.803296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Device: cuda\n",
      "üé≤ Ensemble v·ªõi 4 seeds: [42, 123, 456, 789]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Section 1: IMPORT V√Ä C·∫§U H√åNH\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n\n",
    "TRAIN_SEQ_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta' \n",
    "TRAIN_TERMS_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n",
    "TEST_SEQ_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta'\n",
    "OBO_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_LABELS = 1500\n",
    "\n",
    "# Ensemble config\n",
    "SEEDS = [42, 123, 456, 789]  # 4 seeds kh√°c nhau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üé≤ Ensemble v·ªõi {len(SEEDS)} seeds: {SEEDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T14:44:16.811080Z",
     "iopub.status.busy": "2025-12-09T14:44:16.810786Z",
     "iopub.status.idle": "2025-12-09T14:44:16.830248Z",
     "shell.execute_reply": "2025-12-09T14:44:16.829637Z",
     "shell.execute_reply.started": "2025-12-09T14:44:16.811061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 2: HELPER FUNCTIONS - LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_fasta(path):\n",
    "    \"\"\"ƒê·ªçc file FASTA v√† tr·∫£ v·ªÅ dict {protein_id: sequence}\"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                \n",
    "                header = line[1:]\n",
    "                # Parse ID t·ª´ header\n",
    "                if '|' in header:\n",
    "                    parts = header.split('|')\n",
    "                    current_id = parts[1] if len(parts) > 1 else header.split()[0]\n",
    "                else:\n",
    "                    current_id = header.split()[0]\n",
    "                    \n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "                \n",
    "        if current_id:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "            \n",
    "    return sequences\n",
    "\n",
    "def clean_sequence(seq):\n",
    "    \"\"\"\n",
    "    L√†m s·∫°ch chu·ªói protein:\n",
    "    - Ch·ªâ gi·ªØ 20 amino acids chu·∫©n\n",
    "    - Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    \"\"\"\n",
    "    valid_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "    replacements = {'X': 'A', 'U': 'C', 'B': 'D', 'Z': 'E'}\n",
    "    \n",
    "    cleaned = []\n",
    "    for aa in seq.upper():\n",
    "        if aa in valid_aa:\n",
    "            cleaned.append(aa)\n",
    "        elif aa in replacements:\n",
    "            cleaned.append(replacements[aa])\n",
    "    \n",
    "    return ''.join(cleaned)\n",
    "\n",
    "def is_valid_sequence(seq, min_len=50, max_len=5000):\n",
    "    \"\"\"Ki·ªÉm tra sequence c√≥ h·ª£p l·ªá kh√¥ng\"\"\"\n",
    "    length = len(seq)\n",
    "    if length < min_len or length > max_len:\n",
    "        return False\n",
    "    \n",
    "    valid_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "    valid_count = sum(1 for aa in seq if aa in valid_aa)\n",
    "    \n",
    "    # Y√™u c·∫ßu √≠t nh·∫•t 90% l√† amino acid chu·∫©n\n",
    "    if valid_count / length < 0.9:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T14:44:16.831356Z",
     "iopub.status.busy": "2025-12-09T14:44:16.831159Z",
     "iopub.status.idle": "2025-12-09T14:44:16.848469Z",
     "shell.execute_reply": "2025-12-09T14:44:16.847754Z",
     "shell.execute_reply.started": "2025-12-09T14:44:16.831341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 3: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def get_dipeptide_composition(sequence):\n",
    "    \"\"\"\n",
    "    T·∫°o vector 400 chi·ªÅu t·ª´ t·∫ßn su·∫•t di-peptide\n",
    "    Input: protein sequence\n",
    "    Output: numpy array (400,)\n",
    "    \"\"\"\n",
    "    aa_list = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    aa_map = {aa: i for i, aa in enumerate(aa_list)}\n",
    "    \n",
    "    dipeptide_counts = np.zeros((20, 20), dtype=np.float32)\n",
    "    length = len(sequence)\n",
    "    \n",
    "    if length < 2:\n",
    "        return dipeptide_counts.flatten()\n",
    "    \n",
    "    # ƒê·∫øm t·∫ßn su·∫•t t·ª´ng c·∫∑p amino acid\n",
    "    for i in range(length - 1):\n",
    "        a1 = sequence[i]\n",
    "        a2 = sequence[i + 1]\n",
    "        if a1 in aa_map and a2 in aa_map:\n",
    "            dipeptide_counts[aa_map[a1], aa_map[a2]] += 1\n",
    "    \n",
    "    # Normalize b·∫±ng s·ªë c·∫∑p\n",
    "    return dipeptide_counts.flatten() / (length - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T14:44:16.849328Z",
     "iopub.status.busy": "2025-12-09T14:44:16.849164Z",
     "iopub.status.idle": "2025-12-09T14:44:23.947157Z",
     "shell.execute_reply": "2025-12-09T14:44:23.946539Z",
     "shell.execute_reply.started": "2025-12-09T14:44:16.849314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÇ B∆Ø·ªöC 1: LOAD V√Ä CLEAN D·ªÆ LI·ªÜU\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ ƒêang load train sequences...\n",
      "   ‚úì Loaded: 82404 sequences\n",
      "\n",
      "2Ô∏è‚É£ ƒêang clean sequences...\n",
      "   ‚úì Valid sequences: 81417\n",
      "   ‚úó Invalid sequences: 987\n",
      "   ‚úì Keep ratio: 98.8%\n",
      "\n",
      "3Ô∏è‚É£ ƒêang load train labels...\n",
      "   ‚úì Total annotations: 537027\n",
      "   ‚úì Unique GO terms: 26125\n",
      "\n",
      "4Ô∏è‚É£ ƒêang filter top 1500 GO terms...\n",
      "   ‚úì Proteins v·ªõi labels: 75371\n",
      "\n",
      "üìä TH·ªêNG K√ä:\n",
      "   Sequence length: min=50, max=4998, mean=527\n",
      "   Labels per protein: min=1, max=111, mean=4.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Section 4: PREPROCESSING - LOAD V√Ä CLEAN DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÇ B∆Ø·ªöC 1: LOAD V√Ä CLEAN D·ªÆ LI·ªÜU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load train sequences\n",
    "print(\"\\n1Ô∏è‚É£ ƒêang load train sequences...\")\n",
    "train_seqs_raw = load_fasta(TRAIN_SEQ_PATH)\n",
    "print(f\"   ‚úì Loaded: {len(train_seqs_raw)} sequences\")\n",
    "\n",
    "# Clean sequences\n",
    "print(\"\\n2Ô∏è‚É£ ƒêang clean sequences...\")\n",
    "train_seqs = {}\n",
    "invalid_count = 0\n",
    "\n",
    "for pid, seq in train_seqs_raw.items():\n",
    "    cleaned = clean_sequence(seq)\n",
    "    if is_valid_sequence(cleaned):\n",
    "        train_seqs[pid] = cleaned\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "\n",
    "print(f\"   ‚úì Valid sequences: {len(train_seqs)}\")\n",
    "print(f\"   ‚úó Invalid sequences: {invalid_count}\")\n",
    "print(f\"   ‚úì Keep ratio: {len(train_seqs)/len(train_seqs_raw)*100:.1f}%\")\n",
    "\n",
    "# Load train labels\n",
    "print(\"\\n3Ô∏è‚É£ ƒêang load train labels...\")\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep='\\t')\n",
    "print(f\"   ‚úì Total annotations: {len(train_terms)}\")\n",
    "print(f\"   ‚úì Unique GO terms: {train_terms['term'].nunique()}\")\n",
    "\n",
    "# Filter top terms\n",
    "print(f\"\\n4Ô∏è‚É£ ƒêang filter top {NUM_LABELS} GO terms...\")\n",
    "top_terms = train_terms['term'].value_counts().head(NUM_LABELS).index.tolist()\n",
    "term_to_idx = {term: i for i, term in enumerate(top_terms)}\n",
    "idx_to_term = {i: term for term, i in term_to_idx.items()}\n",
    "\n",
    "train_terms_filtered = train_terms[\n",
    "    (train_terms['term'].isin(top_terms)) &\n",
    "    (train_terms['EntryID'].isin(train_seqs.keys()))\n",
    "]\n",
    "\n",
    "protein_to_labels = train_terms_filtered.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "# Get valid protein IDs (c√≥ c·∫£ sequence v√† labels)\n",
    "valid_ids = [pid for pid in train_seqs.keys() if pid in protein_to_labels]\n",
    "print(f\"   ‚úì Proteins v·ªõi labels: {len(valid_ids)}\")\n",
    "\n",
    "# Statistics\n",
    "seq_lengths = [len(train_seqs[pid]) for pid in valid_ids]\n",
    "label_counts = [len(protein_to_labels[pid]) for pid in valid_ids]\n",
    "\n",
    "print(f\"\\nüìä TH·ªêNG K√ä:\")\n",
    "print(f\"   Sequence length: min={min(seq_lengths)}, max={max(seq_lengths)}, mean={np.mean(seq_lengths):.0f}\")\n",
    "print(f\"   Labels per protein: min={min(label_counts)}, max={max(label_counts)}, mean={np.mean(label_counts):.1f}\")\n",
    "\n",
    "# Free memory\n",
    "del train_seqs_raw, train_terms, train_terms_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T14:44:23.949142Z",
     "iopub.status.busy": "2025-12-09T14:44:23.948788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß B∆Ø·ªöC 2: FEATURE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "üî® ƒêang extract dipeptide features cho t·∫•t c·∫£ proteins...\n",
      "   Progress: 5000/75371 (6.6%)\n",
      "   Progress: 10000/75371 (13.3%)\n",
      "   Progress: 15000/75371 (19.9%)\n",
      "   Progress: 20000/75371 (26.5%)\n",
      "   Progress: 25000/75371 (33.2%)\n",
      "   Progress: 30000/75371 (39.8%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Section 5: FEATURE EXTRACTION CHO TO√ÄN B·ªò DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß B∆Ø·ªöC 2: FEATURE EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüî® ƒêang extract dipeptide features cho t·∫•t c·∫£ proteins...\")\n",
    "all_features = {}\n",
    "\n",
    "for i, pid in enumerate(valid_ids):\n",
    "    seq = train_seqs[pid]\n",
    "    features = get_dipeptide_composition(seq)\n",
    "    all_features[pid] = features\n",
    "    \n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"   Progress: {i+1}/{len(valid_ids)} ({(i+1)/len(valid_ids)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Completed: {len(all_features)} feature vectors (400 dims each)\")\n",
    "\n",
    "# Verify\n",
    "sample_pid = valid_ids[0]\n",
    "print(f\"\\nüìã Sample check:\")\n",
    "print(f\"   Protein ID: {sample_pid}\")\n",
    "print(f\"   Sequence length: {len(train_seqs[sample_pid])}\")\n",
    "print(f\"   Feature shape: {all_features[sample_pid].shape}\")\n",
    "print(f\"   Feature range: [{all_features[sample_pid].min():.4f}, {all_features[sample_pid].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 6: PYTORCH DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class CAFA6Dataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset cho CAFA-6\"\"\"\n",
    "    def __init__(self, protein_ids, features_dict, label_dict, term_map, num_classes):\n",
    "        self.protein_ids = protein_ids\n",
    "        self.features_dict = features_dict\n",
    "        self.label_dict = label_dict\n",
    "        self.term_map = term_map\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.protein_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.protein_ids[idx]\n",
    "        \n",
    "        # Features\n",
    "        features = self.features_dict[pid]\n",
    "        \n",
    "        # Labels (multi-hot encoding)\n",
    "        labels = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        if pid in self.label_dict:\n",
    "            for term in self.label_dict[pid]:\n",
    "                if term in self.term_map:\n",
    "                    labels[self.term_map[term]] = 1.0\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(features, dtype=torch.float32),\n",
    "            torch.tensor(labels, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 7: MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block v·ªõi skip connection\"\"\"\n",
    "    def __init__(self, dim, dropout_rate=0.4):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.layer(x)\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    \"\"\"Residual MLP cho multi-label classification\"\"\"\n",
    "    def __init__(self, input_dim=400, num_classes=1500):\n",
    "        super(ResMLP, self).__init__()\n",
    "        \n",
    "        self.entry = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(1024, dropout_rate=0.4),\n",
    "            ResidualBlock(1024, dropout_rate=0.4)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 8: TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_single_seed(seed, train_ids, val_ids, all_features, protein_to_labels, \n",
    "                      term_to_idx, save_path):\n",
    "    \"\"\"\n",
    "    Train model v·ªõi 1 seed c·ª• th·ªÉ\n",
    "    \n",
    "    Args:\n",
    "        seed: random seed\n",
    "        train_ids: list protein IDs cho training\n",
    "        val_ids: list protein IDs cho validation\n",
    "        all_features: dict features\n",
    "        protein_to_labels: dict labels\n",
    "        term_to_idx: dict term mapping\n",
    "        save_path: path l∆∞u model\n",
    "    \n",
    "    Returns:\n",
    "        best_val_loss: validation loss t·ªët nh·∫•t\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üé≤ TRAINING SEED: {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CAFA6Dataset(\n",
    "        train_ids, all_features, protein_to_labels, term_to_idx, NUM_LABELS\n",
    "    )\n",
    "    val_dataset = CAFA6Dataset(\n",
    "        val_ids, all_features, protein_to_labels, term_to_idx, NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = ResMLP(input_dim=400, num_classes=NUM_LABELS).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"   Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "                  f\"Train: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val: {avg_val_loss:.4f} | \"\n",
    "                  f\"Best: {best_val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Seed {seed} completed. Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"‚úì Model saved to: {save_path}\")\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 9: TRAIN ENSEMBLE (4 SEEDS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ B∆Ø·ªöC 3: TRAINING ENSEMBLE (4 SEEDS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "model_paths = []\n",
    "val_losses = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    # Split data v·ªõi seed n√†y\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        valid_ids, test_size=0.1, random_state=seed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Seed {seed}: Train={len(train_ids)}, Val={len(val_ids)}\")\n",
    "    \n",
    "    # Train\n",
    "    save_path = f'models/model_seed_{seed}.pth'\n",
    "    val_loss = train_single_seed(\n",
    "        seed, train_ids, val_ids, all_features, \n",
    "        protein_to_labels, term_to_idx, save_path\n",
    "    )\n",
    "    \n",
    "    model_paths.append(save_path)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà ENSEMBLE TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for seed, loss in zip(SEEDS, val_losses):\n",
    "    print(f\"   Seed {seed}: Val Loss = {loss:.4f}\")\n",
    "print(f\"\\n   Mean Val Loss: {np.mean(val_losses):.4f}\")\n",
    "print(f\"   Std Val Loss:  {np.std(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 10: LOAD TEST DATA V√Ä EXTRACT FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÇ B∆Ø·ªöC 4: LOAD V√Ä PREPROCESS TEST DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ ƒêang load test sequences...\")\n",
    "test_seqs_raw = load_fasta(TEST_SEQ_PATH)\n",
    "print(f\"   ‚úì Loaded: {len(test_seqs_raw)} sequences\")\n",
    "\n",
    "# Clean test sequences\n",
    "print(\"\\n2Ô∏è‚É£ ƒêang clean test sequences...\")\n",
    "test_seqs = {}\n",
    "for pid, seq in test_seqs_raw.items():\n",
    "    cleaned = clean_sequence(seq)\n",
    "    if is_valid_sequence(cleaned, min_len=10):  # L·ªèng h∆°n cho test\n",
    "        test_seqs[pid] = cleaned\n",
    "\n",
    "print(f\"   ‚úì Valid test sequences: {len(test_seqs)}\")\n",
    "\n",
    "# Extract features\n",
    "print(\"\\n3Ô∏è‚É£ ƒêang extract features cho test set...\")\n",
    "test_features = {}\n",
    "\n",
    "for i, (pid, seq) in enumerate(test_seqs.items()):\n",
    "    features = get_dipeptide_composition(seq)\n",
    "    test_features[pid] = features\n",
    "    \n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"   Progress: {i+1}/{len(test_seqs)} ({(i+1)/len(test_seqs)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Test features ready: {len(test_features)} proteins\")\n",
    "\n",
    "# Free memory\n",
    "del test_seqs_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 11: ENSEMBLE PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÆ B∆Ø·ªöC 5: ENSEMBLE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load all models\n",
    "models = []\n",
    "print(\"\\nüì• Loading models...\")\n",
    "for i, path in enumerate(model_paths):\n",
    "    model = ResMLP(input_dim=400, num_classes=NUM_LABELS).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    print(f\"   ‚úì Model {i+1}/{len(models)} loaded from {path}\")\n",
    "\n",
    "# Predict v·ªõi ensemble\n",
    "print(\"\\nüîÆ ƒêang d·ª± ƒëo√°n v·ªõi ensemble...\")\n",
    "TEMP_SUBMISSION_FILE = 'submission_temp.tsv'\n",
    "\n",
    "with open(TEMP_SUBMISSION_FILE, 'w') as f:\n",
    "    for count, (pid, seq) in enumerate(test_seqs.items(), 1):\n",
    "        features = test_features[pid]\n",
    "        features_tensor = torch.tensor([features], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Ensemble: Average predictions t·ª´ 4 models\n",
    "        ensemble_probs = np.zeros(NUM_LABELS, dtype=np.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                logits = model(features_tensor)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "                ensemble_probs += probs\n",
    "        \n",
    "        # Average\n",
    "        ensemble_probs /= len(models)\n",
    "        \n",
    "        # L·∫•y top 50 predictions\n",
    "        top_indices = np.argsort(ensemble_probs)[::-1][:50]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            score = ensemble_probs[idx]\n",
    "            if score > 0.005:\n",
    "                term_id = idx_to_term[idx]\n",
    "                f.write(f\"{pid}\\t{term_id}\\t{score:.3f}\\n\")\n",
    "        \n",
    "        if count % 2000 == 0:\n",
    "            print(f\"   Progress: {count}/{len(test_seqs)} proteins\")\n",
    "\n",
    "print(f\"\\n‚úì Predictions saved to: {TEMP_SUBMISSION_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 12: POST-PROCESSING (GO HIERARCHY PROPAGATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üå≥ B∆Ø·ªöC 6: POST-PROCESSING (GO Hierarchy)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "FINAL_OUTPUT = 'submission.tsv'\n",
    "\n",
    "# Install goatools n·∫øu c·∫ßn\n",
    "try:\n",
    "    from goatools.obo_parser import GODag\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing goatools...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"goatools\", \"-q\"])\n",
    "    from goatools.obo_parser import GODag\n",
    "\n",
    "if not os.path.exists(OBO_PATH):\n",
    "    print(\"‚ö†Ô∏è  OBO file not found. Using raw predictions.\")\n",
    "    os.rename(TEMP_SUBMISSION_FILE, FINAL_OUTPUT)\n",
    "else:\n",
    "    print(\"\\n1Ô∏è‚É£ Loading GO hierarchy...\")\n",
    "    godag = GODag(OBO_PATH)\n",
    "    print(f\"   ‚úì Loaded {len(godag)} GO terms\")\n",
    "    \n",
    "    def propagate_scores(df_group, godag):\n",
    "        \"\"\"Propagate scores l√™n ancestor terms\"\"\"\n",
    "        current_scores = dict(zip(df_group['GO_Term'], df_group['Score']))\n",
    "        new_scores = current_scores.copy()\n",
    "        \n",
    "        for go_id, score in current_scores.items():\n",
    "            if go_id not in godag:\n",
    "                continue\n",
    "            \n",
    "            term_obj = godag[go_id]\n",
    "            ancestors = term_obj.get_all_parents()\n",
    "            \n",
    "            # Parent ph·∫£i c√≥ score >= child\n",
    "            for ancestor in ancestors:\n",
    "                anc_score_old = new_scores.get(ancestor, 0.0)\n",
    "                new_scores[ancestor] = max(anc_score_old, score)\n",
    "        \n",
    "        return [[pid, term, score] for term, score in new_scores.items() if score >= 0.01]\n",
    "    \n",
    "    # Load temp submission\n",
    "    print(\"\\n2Ô∏è‚É£ Loading predictions...\")\n",
    "    sub_df = pd.read_csv(TEMP_SUBMISSION_FILE, sep='\\t', \n",
    "                         names=['ProteinID', 'GO_Term', 'Score'])\n",
    "    print(f\"   ‚úì Loaded {len(sub_df)} predictions\")\n",
    "    \n",
    "    # Propagate\n",
    "    print(\"\\n3Ô∏è‚É£ Propagating scores...\")\n",
    "    final_data = []\n",
    "    \n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        iterator = tqdm(sub_df.groupby('ProteinID'), desc=\"   Proteins\")\n",
    "    except ImportError:\n",
    "        iterator = sub_df.groupby('ProteinID')\n",
    "        print(\"   (Processing without progress bar)\")\n",
    "    \n",
    "    for pid, group in iterator:\n",
    "        refined_rows = propagate_scores(group, godag)\n",
    "        final_data.extend(refined_rows)\n",
    "    \n",
    "    # Save final submission\n",
    "    print(\"\\n4Ô∏è‚É£ Saving final submission...\")\n",
    "    result_df = pd.DataFrame(final_data, columns=['ProteinID', 'GO_Term', 'Score'])\n",
    "    result_df['Score'] = result_df['Score'].map(lambda x: f'{x:.3f}')\n",
    "    result_df.to_csv(FINAL_OUTPUT, sep='\\t', index=False, header=False)\n",
    "    \n",
    "    print(f\"   ‚úì Total predictions: {len(result_df)}\")\n",
    "    print(f\"   ‚úì Unique proteins: {result_df['ProteinID'].nunique()}\")\n",
    "    print(f\"   ‚úì Unique GO terms: {result_df['GO_Term'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ HO√ÄN T·∫§T! File submission: {FINAL_OUTPUT}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
